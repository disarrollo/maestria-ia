
# Art√≠culos relevantes


## A Multi-Stream Sequence Learning Framework for Human Interaction Recognition (2022)

U. Haroon et al., "A Multi-Stream Sequence Learning Framework for Human Interaction Recognition," in IEEE Transactions on Human-Machine Systems, vol. 52, no. 3, pp. 435-444, June 2022, doi: 10.1109/THMS.2021.3138708.

https://ieeexplore.ieee.org/abstract/document/9695398


## Online Human Interaction Detection and Recognition With Multiple Cameras (2017)

S. Motiian, F. Siyahjani, R. Almohsen and G. Doretto, "Online Human Interaction Detection and Recognition With Multiple Cameras," in IEEE Transactions on Circuits and Systems for Video Technology, vol. 27, no. 3, pp. 649-663, March 2017, doi: 10.1109/TCSVT.2016.2606998.

https://ieeexplore.ieee.org/abstract/document/7563312


## Interactive Phrases: Semantic Descriptions for Human Interaction Recognition (2014)

Y. Kong, Y. Jia and Y. Fu, "Interactive Phrases: Semantic Descriptionsfor Human Interaction Recognition," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 9, pp. 1775-1788, Sept. 2014, doi: 10.1109/TPAMI.2014.2303090.

https://ieeexplore.ieee.org/abstract/document/6739171



## A survey on human activity recognition from videos (2016)

T. Subetha and S. Chitrakala, "A survey on human activity recognition from videos," 2016 International Conference on Information Communication and Embedded Systems (ICICES), 2016, pp. 1-7, doi: 10.1109/ICICES.2016.7518920.

https://ieeexplore.ieee.org/abstract/document/7518920



## Human interaction recognition framework based on interacting body part attention (2022)

https://www.sciencedirect.com/science/article/abs/pii/S0031320322001261



## Discriminative Multi-Modality Speech Recognition (2020)
https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Discriminative_Multi-Modality_Speech_Recognition_CVPR_2020_paper.html

Vision is often used as a complementary modality for audio speech recognition (ASR), especially in the noisy environment where performance of solo audio modality significantly deteriorates. After combining visual modality, ASR is upgraded to the multi-modality speech recognition (MSR). In this paper, we propose a two-stage speech recognition model. In the first stage, the target voice is separated from background noises with help from the corresponding visual information of lip movements, making the model 'listen' clearly. At the second stage, the audio modality combines visual modality again to better understand the speech by a MSR sub-network, further improving the recognition rate. There are some other key contributions: we introduce a pseudo-3D residual convolution (P3D)-based visual front-end to extract more discriminative features; we upgrade the temporal convolution block from 1D ResNet with the temporal convolutional network (TCN), which is more suitable for the temporal tasks; the MSR sub-network is built on the top of Element-wise-Attention Gated Recurrent Unit (EleAtt-GRU), which is more effective than Transformer in long sequences. We conducted extensive experiments on the LRS3-TED and the LRW datasets. Our two-stage model (audio enhanced multi-modality speech recognition, AE-MSR) consistently achieves the state-of-the-art performance by a significant margin, which demonstrates the necessity and effectiveness of AE-MSR.



## Robust Face Frontalization for Visual Speech Recognition (2021)

https://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Kang_Robust_Face_Frontalization_for_Visual_Speech_Recognition_ICCVW_2021_paper.html

Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a robust frontalization method that preserves non-rigid facial deformations, i.e. expressions, to improve lip reading. The method iteratively estimates the rigid transformation (scale, rotation, and translation) and the non-rigid deformation between 3D landmarks extracted from an arbitrarily-viewed face, and 3D vertices parameterized by a deformable shape model. An important merit of the method is its ability to deal with large Gaussian and non-Gaussian errors in the data. For that purpose, we use the generalized Student-t distribution. The associated EM algorithm assigns a weight to each observed landmark, the higher the weight the more important the landmark, thus favoring landmarks that are only affected by rigid head movements. We propose to use the zero-mean normalized cross-correlation (ZNCC) score to evaluate the ability to preserve facial expressions. Moreover, we show that the method, when incorporated into a deep lip-reading pipeline, considerably improves the word classification score on an in-the-wild benchmark.


